{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed7dc12",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 4, Part e: Non-Negative Matrix Factorization DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6df54",
   "metadata": {},
   "source": [
    "- This exercise illustrates usage of **Non-negative Matrix factorization** and covers techniques related to sparse matrices and some basic work with Natural Langauge Processing.  \n",
    "- **We will use NMF to look at the top words for given topics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd8c23",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll be using the BBC dataset. These are articles collected from 5 different topics, with the data pre-processed. \n",
    "\n",
    "These data are available in the data folder (or online [here](http://mlg.ucd.ie/files/datasets/bbc.zip)). The data consists of a few files. The steps we'll be following are:\n",
    "\n",
    "* *bbc.terms* is just a list of words \n",
    "* *bbc.docs* is a list of artcles listed by topic.\n",
    "\n",
    "At a high level, we're going to \n",
    "\n",
    "1. Turn the `bbc.mtx` file into a sparse matrix (a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html) format can be useful for matrices with many values that are 0, and save space by storing the position and values of non-zero elements).\n",
    "1. Decompose that sparse matrix using NMF.\n",
    "1. Use the resulting components of NMF to analyze the topics that result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada6955",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e861c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/bbc.mtx\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ade8fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9635 2225 286774\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.pop(0)\n",
    "content.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98717738",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Here, we will turn this into a list of tuples representing a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html). Remember the description of the file from above:\n",
    "\n",
    "* *bbc.mtx* is a list: first column is **wordID**, second is **articleID** and the third is the number of times that word appeared in that article.\n",
    "\n",
    "So, if word 1 appears in article 3, 2 times, one element of our list will be:\n",
    "\n",
    "`(1, 3, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5cdec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1),\n",
       " (1, 7, 2),\n",
       " (1, 11, 1),\n",
       " (1, 14, 1),\n",
       " (1, 15, 2),\n",
       " (1, 19, 2),\n",
       " (1, 21, 1),\n",
       " (1, 29, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsemat = [tuple(map(int,map(float,c.split()))) for c in content]\n",
    "# Let's examine the first few elements\n",
    "sparsemat[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe743eb",
   "metadata": {},
   "source": [
    "## Part 2: Preparing Sparse Matrix data for NMF \n",
    "We will use the [coo matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to turn the sparse matrix into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f62e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "rows = [x[0] for x in sparsemat]\n",
    "cols = [x[1] for x in sparsemat]\n",
    "values = [x[2] for x in sparsemat]\n",
    "coo = coo_matrix((values, (rows, cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471de78b",
   "metadata": {},
   "source": [
    "## NMF\n",
    "- NMF is a way of decomposing a matrix of documents and words so that one of the matrices can be interpreted as the \"loadings\" or \"weights\" of each word on a topic. \n",
    "    - Check out [the NMF documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) and the [examples of topic extraction using NMF and LDA](http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf556a3",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Here, we will import `NMF`, define a model object with 5 components, and `fit_transform` the data created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70b619a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9636, 2226)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c517a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6276ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9636, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NMF(n_components=5, init=\"random\", random_state=818)\n",
    "doc_topic = model.fit_transform(coo)\n",
    "\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b172cbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find feature with highest value per doc\n",
    "np.argmax(doc_topic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d391233",
   "metadata": {},
   "source": [
    "## Part 4: \n",
    "\n",
    "Check out the `components` of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84de6851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2226)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051498d",
   "metadata": {},
   "source": [
    "- This is five rows, each of which is a \"topic\" containing the weights of each word on that topic. \n",
    "- The exercise is to _get a list of the top 10 words for each topic_. \n",
    "- We can just store this in a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f86b2",
   "metadata": {},
   "source": [
    "**Note:** Just like we read in the data above, we'll have to read in the words from the `bbc.terms` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344efb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/bbc.terms\") as f:\n",
    "    content = f.readlines()\n",
    "words = [c.split()[0] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb054a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for r in model.components_:\n",
    "    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:12]\n",
    "    topic_words.append([words[e[1]] for e in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e47d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bondi',\n",
       "  'stanlei',\n",
       "  'continent',\n",
       "  'mortgag',\n",
       "  'bare',\n",
       "  'least',\n",
       "  'extent',\n",
       "  '200',\n",
       "  'leav',\n",
       "  'frustrat',\n",
       "  'yuan',\n",
       "  'industri'],\n",
       " ['manipul',\n",
       "  'teenag',\n",
       "  'drawn',\n",
       "  'go',\n",
       "  'prosecutor',\n",
       "  'herbert',\n",
       "  'host',\n",
       "  'protest',\n",
       "  'hike',\n",
       "  'nation',\n",
       "  'calcul',\n",
       "  'power'],\n",
       " ['dimens',\n",
       "  'hous',\n",
       "  'march',\n",
       "  'wider',\n",
       "  'owner',\n",
       "  'intend',\n",
       "  'declin',\n",
       "  'forc',\n",
       "  'posit',\n",
       "  'founder',\n",
       "  'york',\n",
       "  'unavail'],\n",
       " ['rome',\n",
       "  'ft',\n",
       "  'regain',\n",
       "  'lawmak',\n",
       "  'outright',\n",
       "  'resum',\n",
       "  'childhood',\n",
       "  'greatest',\n",
       "  'citi',\n",
       "  'stagnat',\n",
       "  'crown',\n",
       "  'bodi'],\n",
       " ['build',\n",
       "  'empir',\n",
       "  'isol',\n",
       "  'Â£12',\n",
       "  'restructur',\n",
       "  'closer',\n",
       "  'plung',\n",
       "  'depreci',\n",
       "  'durham',\n",
       "  'race',\n",
       "  'juli',\n",
       "  'segreg']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, each set of words relates to the corresponding topic (ie the first set of words relates to topic 'Business', etc.)\n",
    "topic_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168da7f7",
   "metadata": {},
   "source": [
    "The original data had 5 topics, as listed in `bbc.docs` (which these topic words relate to). \n",
    "\n",
    "```\n",
    "Business\n",
    "Entertainment\n",
    "Politics\n",
    "Sport\n",
    "Tech\n",
    "```\n",
    "\n",
    "- In \"real life\", we would have found a way to use these to inform the model.\n",
    "- But for this little demo, we can just compare the recovered topics to the original ones. \n",
    "    - And they seem to match reasonably well. \n",
    "    - The order is different, which is to be expected in this kind of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a8ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business.001\\n',\n",
       " 'business.002\\n',\n",
       " 'business.003\\n',\n",
       " 'business.004\\n',\n",
       " 'business.005\\n',\n",
       " 'business.006\\n',\n",
       " 'business.007\\n',\n",
       " 'business.008\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/bbc.docs') as d:\n",
    "    doc_content = d.readlines()\n",
    "    \n",
    "doc_content[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c00e4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd048f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
