{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a67bc5f",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part i: Reinforcement Learning DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecff9c",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Example\n",
    "\n",
    "In this example from Reinforcement Learning, the task is to use tools from Machine Learning to predict how an agent should act. We will then use those predictions to drive the behavior of the agent. Ideally, our intelligent agent should get a much better score than a random agent.\n",
    "\n",
    "## Key concepts:\n",
    "\n",
    "- **Observation**: These are the states of the game. It describes where the agent currently is.\n",
    "- **Action**: These are the moves that the agent makes.\n",
    "- **Episode**: One full game played from beginning (`env.reset()`) to end (when `done == True`).\n",
    "- **Step**: Part of a game that includes one action. The game transitions from one observation to the next.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This exaple uses the Python library [OpenAI Gym](https://gym.openai.com/docs/).\n",
    "\n",
    "If you want to install everything (gym can run atari games.) follow [these instructions](https://github.com/openai/gym#installing-everything).\n",
    "\n",
    "Now we can build an environment using OpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dbd3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33ffdd",
   "metadata": {},
   "source": [
    "# The first part of the game uses the environment FrozenLake-V1\n",
    "\n",
    "- Winter is here. \n",
    "- You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. \n",
    "- The water is mostly frozen, but there are a few holes where the ice has melted. \n",
    "- If you step into one of those holes, you'll fall into the freezing water. \n",
    "- At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. \n",
    "- However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "This is a small world with 16 tiles. <br>\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "    SFFF (S: starting point, safe)\n",
    "    FHFH (F: frozen surface, safe)\n",
    "    FFFH (H: hole, fall to your doom)\n",
    "    HFFG (G: goal, where the frisbee is located)\n",
    "\n",
    "#### The game starts at the S tile. The object of the game is to get to the goal (G) without landing in a hole (H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6ece6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Build an environment with gym.make()\n",
    "env = gym.make(\"FrozenLake-v1\") # build a fresh environment\n",
    "\n",
    "# Start a new game with env.reset()\n",
    "current_observation = env.reset() # this starts a new \"episode\" and returns the initial observation.\n",
    "\n",
    "#the current observation is just the current location\n",
    "print(current_observation) # observations are just a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adda0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# we can print the environment if we want to look at it\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2b3eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our action space: Discrete(4)\n",
      "Our new action: 3\n"
     ]
    }
   ],
   "source": [
    "# the action space for this environment includes four discrete actions\n",
    "print(f\"Our action space: {env.action_space}\")\n",
    "\n",
    "new_action = env.action_space.sample() # We can randomly sample actions.\n",
    "\n",
    "print(f\"Our new action: {new_action}\") # run this cell a few times to get an idea of the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3686e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 4, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# now we act! do this with the step function\n",
    "\n",
    "new_action = env.action_space.sample()\n",
    "\n",
    "observation, reward, done, info = env.step(new_action)\n",
    "\n",
    "# here's a look at what we get back\n",
    "print(f\"Observation: {observation}, reward: {reward}, done: {done}, info: {info}\")\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94418d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: 4, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# we can put this process into a for-loop and see how the game progresses\n",
    "\n",
    "current_observation = env.reset() # start a new game\n",
    "\n",
    "for i in range(5): # run 5 moves\n",
    "\n",
    "    new_action = env.action_space.sample() # sample a new action\n",
    "\n",
    "    observation, reward, done, info = env.step(new_action) # step through the action and get the outputs\n",
    "\n",
    "    # here's a look at what we get back\n",
    "    print(f\"observation: {observation}, reward: {reward}, done: {done}, info: {info}\")\n",
    "\n",
    "    env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd504c9",
   "metadata": {},
   "source": [
    "Now we can guess what each of the outputs mean. \n",
    "\n",
    "**Observation** refers to the number of the tile. The tiles appear to be numbered\n",
    "\n",
    "    0 1 2 3\n",
    "    4 5 ...\n",
    "    \n",
    "**Reward** refers to the outcome of the game. We get 1 if we win, zero otherwise.\n",
    "\n",
    "**Done** tells us if the game is still going. It goes to true when we win or fall into a hole.\n",
    "\n",
    "**info** gives extra info about the world. Here, it's probabilities. Can you guess what this means here? Perhaps the world is a bit noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734849b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:1 observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:3 observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:3 observation: 0, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:1 observation: 1, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:1 observation: 2, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:1 observation: 1, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:1 observation: 2, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:0 observation: 6, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:2 observation: 2, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:2 observation: 2, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:3 observation: 3, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:2 observation: 3, reward: 0.0, done: False, info: {'prob': 0.3333333333333333}\n",
      "action:2 observation: 7, reward: 0.0, done: True, info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# Here's how to simulate an entire episode\n",
    "# We're going to stop rendering it every time to save space\n",
    "\n",
    "current_observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    new_action = env.action_space.sample()\n",
    "    new_observation, reward, done, info = env.step(new_action)\n",
    "    print(f\"action:{new_action} observation: {new_observation}, reward: {reward}, done: {done}, info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52b7df",
   "metadata": {},
   "source": [
    "The environment has some squares that always end the game (`H` in the render), some that don't (`F`), and one that is presumably the reward, if you get to it.\n",
    "\n",
    "The actions seem like up, down, left right. But they also seem stochastic. There seems to be a 1/3 chance of going into 3 different squares with each action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee484f22",
   "metadata": {},
   "source": [
    "# Part 1: Gather data\n",
    "\n",
    "We want to build an intelligent actor but first we have to gather data on which actions are useful.\n",
    "\n",
    "Use the above code as reference. Run a *random* agent through 1,000 or more episodes and collect data on each step.\n",
    "\n",
    "I recommend you store this data in a pandas dataframe. Each row should be a step. Your features should include the following features or similar \n",
    "\n",
    "- `observation` the observation at the beginning of the step (before acting!)\n",
    "- `action` the action randomly sampled\n",
    "- `current_reward` the reward received after the action was performed\n",
    "\n",
    "After you generate this data, it is recommended that you compute a column (e.g. `total_reward` that is the total reward for the entire episode).\n",
    "\n",
    "At the end of the data gathering, you should be able to use pandas (or similar) to calculate the average total reward *per episode* of the random agent. The average score should be 1-2%, meaning that the agent very rarely wins.\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "- `initial_observation = env.reset()` starts a new episode and returns the initial observation.\n",
    "- `new_observation, reward, done, info = env.step(new_action)` executes one action and returns the following observation. You may look at the documentation for the step method if you are curious about what it does. \n",
    "- `done != True` until the game is finished.\n",
    "- we are trying to maximize the reward *per episode*. Our first game gives 0 reward unless the agent travels to the goal.\n",
    "- `env.action_space.n` gives the number of possible actions in the environment. `env.action_space.sample()` allows the agent to randomly sample an action.\n",
    "- `env.observation_space.n` gives the number of possible states in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2416a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "num_episodes = 40000\n",
    "\n",
    "life_memory = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # start a new episode and record all the memories\n",
    "    old_observation = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    ep_memory = []\n",
    "    while not done:\n",
    "        new_action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(new_action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        ep_memory.append({\n",
    "            \"observation\": old_observation,\n",
    "            \"action\": new_action,\n",
    "            \"reward\": reward,\n",
    "            \"episode\": i\n",
    "        })\n",
    "        old_observation = observation\n",
    "    \n",
    "    # incorporate total reward\n",
    "    num_steps = len(ep_memory)\n",
    "    for i, ep_mem in enumerate(ep_memory):\n",
    "        ep_mem[\"tot_reward\"] = tot_reward\n",
    "        ep_mem[\"decay_reward\"] = i*tot_reward/num_steps\n",
    "    \n",
    "    life_memory.extend(ep_memory)\n",
    "    \n",
    "memory_df = pd.DataFrame(life_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca33ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>episode</th>\n",
       "      <th>tot_reward</th>\n",
       "      <th>decay_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   observation  action  reward  episode  tot_reward  decay_reward\n",
       "0            0       0     0.0        0         0.0           0.0\n",
       "1            4       1     0.0        0         0.0           0.0\n",
       "2            4       0     0.0        0         0.0           0.0\n",
       "3            8       2     0.0        0         0.0           0.0\n",
       "4            4       1     0.0        0         0.0           0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fd8e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>episode</th>\n",
       "      <th>tot_reward</th>\n",
       "      <th>decay_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307721.000000</td>\n",
       "      <td>307721.000000</td>\n",
       "      <td>307721.000000</td>\n",
       "      <td>307721.000000</td>\n",
       "      <td>307721.000000</td>\n",
       "      <td>307721.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.233107</td>\n",
       "      <td>1.495787</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>19987.229442</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.010589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.010266</td>\n",
       "      <td>1.117480</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>11585.843486</td>\n",
       "      <td>0.149742</td>\n",
       "      <td>0.081724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30028.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         observation         action         reward        episode  \\\n",
       "count  307721.000000  307721.000000  307721.000000  307721.000000   \n",
       "mean        2.233107       1.495787       0.001771   19987.229442   \n",
       "std         3.010266       1.117480       0.042047   11585.843486   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000    9949.000000   \n",
       "50%         1.000000       1.000000       0.000000   19951.000000   \n",
       "75%         4.000000       2.000000       0.000000   30028.000000   \n",
       "max        14.000000       3.000000       1.000000   39999.000000   \n",
       "\n",
       "          tot_reward   decay_reward  \n",
       "count  307721.000000  307721.000000  \n",
       "mean        0.022949       0.010589  \n",
       "std         0.149742       0.081724  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       0.970588  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3eab4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307721, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5235c7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.groupby(\"episode\").reward.sum().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ab17d",
   "metadata": {},
   "source": [
    "# Step 2: Predict\n",
    "\n",
    "- Now that you have a bunch of data, put it into a format that you can model. \n",
    "- The goal here is to guide the behavior of our agent.\n",
    "- Our agent will be given an observation and need to decide between the possible actions given that observation and the prediction of the model. \n",
    "\n",
    "- So, you will need to design a quantity that you can ask your model to predict for every possible action-observation pair. \n",
    "- Think a bit about what this quantity should be.\n",
    "- Should the model try to predict the immediate reward for each action? \n",
    "    - If so, how would it know where to go at the beginning of each episode when all moves give zero reward but when some moves bring it closer to the goal than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acef873b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(n_estimators=50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=50)\n",
    "# model = SVR()\n",
    "y = 0.5*memory_df.reward + 0.1*memory_df.decay_reward + memory_df.tot_reward\n",
    "x = memory_df[[\"observation\", \"action\"]]\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ab895",
   "metadata": {},
   "source": [
    "# Step 3: Act\n",
    "\n",
    "Now that you have a model that predicts the desired behavior, let's act on it! Modify the code you used to gather data so that you replace the random decision with an intelligent one.\n",
    "\n",
    "We started out winning ~1.5% of the games with the random agent. How well can you do? You should be able to get your model to do at least 10x better (so 15%). Can you get ~50%?\n",
    "\n",
    "If you're having trouble, tune your model. Try different representations of the observation and action spaces. Try different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a1ccf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "y = 1*memory_df.reward + memory_df.tot_reward + .1*memory_df.decay_reward\n",
    "x = memory_df[[\"observation\", \"action\"]]\n",
    "model.fit(x, y)\n",
    "\n",
    "num_episodes = 500\n",
    "random_per = 0\n",
    "\n",
    "life_memory = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # start a new episode and record all the memories\n",
    "    old_observation = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    ep_memory = []\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < random_per:\n",
    "            new_action = env.action_space.sample()\n",
    "        else:\n",
    "            pred_in = [[old_observation,i] for i in range(4)]\n",
    "            new_action = np.argmax(model.predict(pred_in))\n",
    "        observation, reward, done, info = env.step(new_action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        ep_memory.append({\n",
    "            \"observation\": old_observation,\n",
    "            \"action\": new_action,\n",
    "            \"reward\": reward,\n",
    "            \"episode\": i,\n",
    "        })\n",
    "        old_observation = observation\n",
    "        \n",
    "    # incorporate total reward\n",
    "    for ep_mem in ep_memory:\n",
    "        ep_mem[\"tot_reward\"] = tot_reward\n",
    "        \n",
    "    life_memory.extend(ep_memory)\n",
    "    \n",
    "memory_df2 = pandas.DataFrame(life_memory)\n",
    "\n",
    "# rf.fit(memory_df[[\"observation\", \"action\"]], memory_df[\"comb_reward\"])\n",
    "\n",
    "# score\n",
    "# much better!\n",
    "memory_df2.groupby(\"episode\").reward.sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de35b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = .1*memory_df.reward + 1*memory_df.decay_reward + 1*memory_df.tot_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f097cfc",
   "metadata": {},
   "source": [
    "# Pole cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc9d25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a7ea72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.583"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can build a toy world!\n",
    "num_episodes = 1000\n",
    "\n",
    "life_memory = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # start a new episode and record all the memories\n",
    "    old_observation = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    ep_memory = []\n",
    "    while not done:\n",
    "        new_action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(new_action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        ep_memory.append({\n",
    "            \"obs0\": old_observation[0],\n",
    "            \"obs1\": old_observation[1],\n",
    "            \"obs2\": old_observation[2],\n",
    "            \"obs3\": old_observation[3],\n",
    "            \"action\": new_action,\n",
    "            \"reward\": reward,\n",
    "            \"episode\": i,\n",
    "        })\n",
    "        old_observation = observation\n",
    "        \n",
    "    # incorporate total reward\n",
    "    for ep_mem in ep_memory:\n",
    "        ep_mem[\"tot_reward\"] = tot_reward\n",
    "        \n",
    "    life_memory.extend(ep_memory)\n",
    "    \n",
    "memory_df = pandas.DataFrame(life_memory)\n",
    "\n",
    "memory_df.groupby(\"episode\").reward.sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009d483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs0</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>episode</th>\n",
       "      <th>tot_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023473</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.036469</td>\n",
       "      <td>-0.011991</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023394</td>\n",
       "      <td>-0.198550</td>\n",
       "      <td>-0.036709</td>\n",
       "      <td>0.268966</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019423</td>\n",
       "      <td>-0.393130</td>\n",
       "      <td>-0.031329</td>\n",
       "      <td>0.549848</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011560</td>\n",
       "      <td>-0.587798</td>\n",
       "      <td>-0.020332</td>\n",
       "      <td>0.832498</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.392404</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>0.533491</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       obs0      obs1      obs2      obs3  action  reward  episode  tot_reward\n",
       "0  0.023473 -0.003970 -0.036469 -0.011991       0     1.0        0        24.0\n",
       "1  0.023394 -0.198550 -0.036709  0.268966       0     1.0        0        24.0\n",
       "2  0.019423 -0.393130 -0.031329  0.549848       0     1.0        0        24.0\n",
       "3  0.011560 -0.587798 -0.020332  0.832498       1     1.0        0        24.0\n",
       "4 -0.000196 -0.392404 -0.003682  0.533491       0     1.0        0        24.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80974972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs0</th>\n",
       "      <th>obs1</th>\n",
       "      <th>obs2</th>\n",
       "      <th>obs3</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>episode</th>\n",
       "      <th>tot_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.0</td>\n",
       "      <td>22583.000000</td>\n",
       "      <td>22583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.022177</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>496.153965</td>\n",
       "      <td>29.662357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.091836</td>\n",
       "      <td>0.533777</td>\n",
       "      <td>0.090375</td>\n",
       "      <td>0.788631</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.569602</td>\n",
       "      <td>17.705580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.775363</td>\n",
       "      <td>-2.056873</td>\n",
       "      <td>-0.209421</td>\n",
       "      <td>-2.736892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.040149</td>\n",
       "      <td>-0.348090</td>\n",
       "      <td>-0.053960</td>\n",
       "      <td>-0.547061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>-0.008472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.042530</td>\n",
       "      <td>0.371508</td>\n",
       "      <td>0.050672</td>\n",
       "      <td>0.478134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.091120</td>\n",
       "      <td>2.427556</td>\n",
       "      <td>0.209403</td>\n",
       "      <td>2.719097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>127.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               obs0          obs1          obs2          obs3        action  \\\n",
       "count  22583.000000  22583.000000  22583.000000  22583.000000  22583.000000   \n",
       "mean       0.002786      0.017990     -0.002022     -0.022177      0.502900   \n",
       "std        0.091836      0.533777      0.090375      0.788631      0.500003   \n",
       "min       -0.775363     -2.056873     -0.209421     -2.736892      0.000000   \n",
       "25%       -0.040149     -0.348090     -0.053960     -0.547061      0.000000   \n",
       "50%        0.000349      0.003838     -0.001501     -0.008472      1.000000   \n",
       "75%        0.042530      0.371508      0.050672      0.478134      1.000000   \n",
       "max        1.091120      2.427556      0.209403      2.719097      1.000000   \n",
       "\n",
       "        reward       episode    tot_reward  \n",
       "count  22583.0  22583.000000  22583.000000  \n",
       "mean       1.0    496.153965     29.662357  \n",
       "std        0.0    288.569602     17.705580  \n",
       "min        1.0      0.000000      8.000000  \n",
       "25%        1.0    247.000000     17.000000  \n",
       "50%        1.0    494.000000     25.000000  \n",
       "75%        1.0    744.000000     37.000000  \n",
       "max        1.0    999.000000    127.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b08be35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(n_estimators=50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=50)\n",
    "\n",
    "memory_df[\"comb_reward\"] = .5*memory_df.reward + memory_df.tot_reward\n",
    "model.fit(memory_df[[\"obs0\", \"obs1\", \"obs2\", \"obs3\", \"action\"]], memory_df.comb_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7754fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.37"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "random_per = 0\n",
    "\n",
    "life_memory = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # start a new episode and record all the memories\n",
    "    old_observation = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    ep_memory = []\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < random_per:\n",
    "            new_action = env.action_space.sample()\n",
    "        else:\n",
    "            pred_in = [list(old_observation)+[i] for i in range(2)]\n",
    "            new_action = np.argmax(model.predict(pred_in))\n",
    "        observation, reward, done, info = env.step(new_action)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        ep_memory.append({\n",
    "            \"obs0\": old_observation[0],\n",
    "            \"obs1\": old_observation[1],\n",
    "            \"obs2\": old_observation[2],\n",
    "            \"obs3\": old_observation[3],\n",
    "            \"action\": new_action,\n",
    "            \"reward\": reward,\n",
    "            \"episode\": i,\n",
    "        })\n",
    "        old_observation = observation\n",
    "        \n",
    "    # incorporate total reward\n",
    "    for ep_mem in ep_memory:\n",
    "        ep_mem[\"tot_reward\"] = tot_reward\n",
    "        \n",
    "    life_memory.extend(ep_memory)\n",
    "    \n",
    "memory_df2 = pandas.DataFrame(life_memory)\n",
    "memory_df2[\"comb_reward\"] = memory_df2.reward + memory_df2.tot_reward\n",
    "\n",
    "# score\n",
    "# much better!\n",
    "memory_df2.groupby(\"episode\").reward.sum().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720aa51",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
